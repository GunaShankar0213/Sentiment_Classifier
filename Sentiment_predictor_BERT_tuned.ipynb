{
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KW6b0DYk4DgC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "VHhxWu5D4Dwf",
        "outputId": "c5c91b53-5f16-4a6e-9a94-78567fee47cc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "!pip install transformers\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Load the dataset into a pandas DataFrame\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/EDA/Ai Prompt Generator/Ai Writting/Conversation/topical_chat.csv\")  # Replace with your dataset path\n",
        "X = df['message']\n",
        "y = df['sentiment']\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Load the pre-trained BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "\n",
        "# Tokenize and encode the text data\n",
        "def tokenize_text(text):\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "\n",
        "    for sent in text:\n",
        "        encoded_dict = tokenizer.encode_plus(\n",
        "            sent,\n",
        "            add_special_tokens=True,\n",
        "            max_length=128,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        input_ids.append(encoded_dict['input_ids'])\n",
        "        attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "    input_ids = torch.cat(input_ids, dim=0)\n",
        "    attention_masks = torch.cat(attention_masks, dim=0)\n",
        "\n",
        "    return input_ids, attention_masks\n",
        "\n",
        "# Tokenize the training and validation text data\n",
        "train_inputs, train_masks = tokenize_text(X_train)\n",
        "val_inputs, val_masks = tokenize_text(X_val)\n"
      ],
      "metadata": {
        "id": "cn9dVD5_4AJW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Initialize the LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Fit and transform the sentiment labels\n",
        "train_labels = torch.tensor(label_encoder.fit_transform(y_train))\n",
        "val_labels = torch.tensor(label_encoder.transform(y_val))"
      ],
      "metadata": {
        "id": "7k-d7tBY51DY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Label ID: 0 => Original Name:  Angry\\\n",
        "Label ID: 1 => Original Name:  Curious to dive deeper\\\n",
        "Label ID: 2 => Original Name:  Disgusted\\\n",
        "Label ID: 3 => Original Name:  Fearful\\\n",
        "Label ID: 4 => Original Name:  Happy\\\n",
        "Label ID: 5 => Original Name:  Neutral\\\n",
        "Label ID: 6 => Original Name:  Sad\\\n",
        "Label ID: 7 => Original Name:  Surprised\\\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GDq1xsiK99s1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1   Curious to dive deeper\\\n",
        "2   sad\\\n",
        "3   FearFul \\\n",
        "4   Neutral\t\\\n",
        "5   Happy\\\n",
        "6   Angry\\\n",
        "7   Surprised\\\n",
        "8   Disgusted\n"
      ],
      "metadata": {
        "id": "LtJO7A6bmiDp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a DataLoader for efficient batch processing\n",
        "batch_size = 32\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "val_data = TensorDataset(val_inputs, val_masks, val_labels)\n",
        "val_sampler = SequentialSampler(val_data)\n",
        "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)\n"
      ],
      "metadata": {
        "id": "QDG3Dwpv6DWV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pre-trained BERT model with a classification head\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(df['sentiment'].unique()))\n",
        "\n",
        "# Set the device to GPU if available, else CPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "# Set up the optimizer and learning rate scheduler\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "total_steps = len(train_dataloader) * 4  # 10 epochs\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9)"
      ],
      "metadata": {
        "id": "baVzLMML6GQE",
        "outputId": "7553e575-5871-4f3a-afd8-7e585f853841",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fine-tuning the BERT model\n",
        "print(\"Fine-tuning BERT...\")\n",
        "for epoch in range(4):  # Adjust the number of epochs as needed\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}\", unit=\"batch\"):\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        inputs, masks, labels = batch\n",
        "\n",
        "        model.zero_grad()\n",
        "\n",
        "        outputs = model(inputs, attention_mask=masks, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        logits = outputs.logits\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "    avg_loss = total_loss / len(train_dataloader)\n",
        "    print(f\"Avg training loss: {avg_loss:.4f}\")\n",
        "\n",
        "    # Evaluation on the validation set\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    val_accuracy = 0\n",
        "\n",
        "    for batch in val_dataloader:\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        inputs, masks, labels = batch\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(inputs, attention_mask=masks, labels=labels)\n",
        "            loss = outputs.loss\n",
        "            logits = outputs.logits\n",
        "\n",
        "        val_loss += loss.item()\n",
        "        val_accuracy += (logits.argmax(dim=1) == labels).sum().item()\n",
        "\n",
        "    avg_val_loss = val_loss / len(val_dataloader)\n",
        "    val_accuracy = val_accuracy / len(X_val)\n",
        "    print(f\"Avg validation loss: {avg_val_loss:.4f}\")\n",
        "    print(f\"Validation accuracy: {val_accuracy:.4f}\")\n",
        "\n",
        "# Save the fine-tuned model\n",
        "model.save_pretrained(\"fine_tuned_bert_model\")\n",
        "tokenizer.save_pretrained(\"fine_tuned_bert_model\")"
      ],
      "metadata": {
        "id": "lmWvN_LQ4VkI",
        "outputId": "c7c994c4-4e8f-4bc0-87de-b99e7af74b9d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fine-tuning BERT...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1: 100%|██████████| 4710/4710 [52:06<00:00,  1.51batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg training loss: 1.7184\n",
            "Avg validation loss: 1.6921\n",
            "Validation accuracy: 0.4228\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2: 100%|██████████| 4710/4710 [52:04<00:00,  1.51batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg training loss: 1.7178\n",
            "Avg validation loss: 1.6921\n",
            "Validation accuracy: 0.4228\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3: 100%|██████████| 4710/4710 [52:04<00:00,  1.51batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg training loss: 1.7172\n",
            "Avg validation loss: 1.6921\n",
            "Validation accuracy: 0.4228\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4: 100%|██████████| 4710/4710 [52:06<00:00,  1.51batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg training loss: 1.7168\n",
            "Avg validation loss: 1.6921\n",
            "Validation accuracy: 0.4228\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('fine_tuned_bert_model/tokenizer_config.json',\n",
              " 'fine_tuned_bert_model/special_tokens_map.json',\n",
              " 'fine_tuned_bert_model/vocab.txt',\n",
              " 'fine_tuned_bert_model/added_tokens.json')"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"/content/drive/MyDrive/EDA/Ai Prompt Generator/Ai Writting/fine_tuned_bert_model\")\n",
        "tokenizer.save_pretrained(\"/content/drive/MyDrive/EDA/Ai Prompt Generator/Ai Writting/fine_tuned_bert_model\")"
      ],
      "metadata": {
        "id": "oY9gLIqr6Lqc",
        "outputId": "ee43e79b-392e-4e74-9af6-83f22d3c25c2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/drive/MyDrive/EDA/Ai Prompt Generator/Ai Writting/fine_tuned_bert_model/tokenizer_config.json',\n",
              " '/content/drive/MyDrive/EDA/Ai Prompt Generator/Ai Writting/fine_tuned_bert_model/special_tokens_map.json',\n",
              " '/content/drive/MyDrive/EDA/Ai Prompt Generator/Ai Writting/fine_tuned_bert_model/vocab.txt',\n",
              " '/content/drive/MyDrive/EDA/Ai Prompt Generator/Ai Writting/fine_tuned_bert_model/added_tokens.json')"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jXkOFwxuvyPR"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}